# Autonomous Multi-Agent Business Intelligence System

AI-powered BI assistant that turns natural-language questions into SQL, runs optional analytics and external research, streams proactive anomaly alerts, and can generate executive-friendly outputs.

This repo contains two UIs:
- A general Streamlit dashboard (recommended for most users): `app/streamlit_ui.py`
- An â€œExecutive Boardroomâ€ dashboard with a Legacy Mode toggle (v2 vs v3): `src/ui/boardroom_app.py`

## Whatâ€™s in this repo (verified)

### Backend APIs

**Phase 4 API (recommended for the dashboards)**
- Entry: `src.api.main_crewai:app`
- Includes:
    - `POST /query`
    - `POST /query/analytics`
    - `POST /query/research`
    - `GET /alerts/recent`
    - `POST /alerts/check/{metric_name}`
    - `WS /ws/alerts`
    - `GET /health`

**Core API (alternative / lighter orchestration)**
- Entry: `src.api.main:app`
- Includes `POST /query` with optional `agentic` routing.

### Frontend UIs

**Streamlit Dashboard**
- Entry: `app/streamlit_ui.py`
- Tabs for querying, monitoring, history, settings.
- Uses the Phase 4 API endpoints above.

**Executive Boardroom (Legacy Mode)**
- Entry: `src/ui/boardroom_app.py`
- Sidebar â€œOperation Modeâ€:
    - **Simple Query (v2.0)**: fast report layout, no alerts/voice
    - **Autonomous Briefing (v3.0)**: executive layout + briefing

## Requirements

- Python **3.11+** (see `pyproject.toml`)
- Windows / macOS / Linux

## Setup

### 1) Create and activate a virtual environment

Windows (PowerShell):

```powershell
python -m venv .venv
& .\.venv\Scripts\Activate.ps1
```

macOS/Linux:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

Optional (recommended if you use the NLP components):

```bash
python -m spacy download en_core_web_sm
```

### 3) Configure environment

Create `.env` in the repo root.

Required:
- `GROQ_API_KEY` (Phase 4 backend uses this at startup)

Optional:
- `OPENAI_API_KEY` (reasoning/critic; can be disabled)
- `TAVILY_API_KEY` (external research; research mode degrades gracefully if unset)

Flags:
- `DATAGENIE_DISABLE_OPENAI=1` to force OpenAI off
- `DATAGENIE_TEST_MODE=1` to reduce side effects during tests

### 4) (Optional) Initialize sample data

These scripts exist in `scripts/` and can be used to seed the local demo data:

```bash
python scripts/create_sample_data.py
python scripts/init_vector_store.py
```

## Run

### Option A: Run the recommended Phase 4 backend + Streamlit Dashboard

Terminal 1 (backend):

```bash
python -m uvicorn src.api.main_crewai:app --host 127.0.0.1 --port 8000 --reload
```

Terminal 2 (frontend):

```bash
python -m streamlit run app/streamlit_ui.py --server.port 8501 --server.address 127.0.0.1
```

Open:
- UI: http://127.0.0.1:8501
- API health: http://127.0.0.1:8000/health
- API docs: http://127.0.0.1:8000/docs

### Option B: Run the Executive Boardroom (Legacy Mode toggle)

Backend (same as above):

```bash
python -m uvicorn src.api.main_crewai:app --host 127.0.0.1 --port 8000 --reload
```

Frontend:

```bash
python -m streamlit run src/ui/boardroom_app.py --server.port 8501
```

### Option C: One-command launcher

This starts `src.api.main_crewai:app` and `app/streamlit_ui.py`:

```bash
python scripts/launch_datagenie.py
```

## Troubleshooting

### Backend wonâ€™t import `src`

Run from the repo root (the folder containing `src/`). If you launch uvicorn from another working directory, Python may not resolve imports.

### Missing spaCy model

If you see spaCy errors about `en_core_web_sm`, install it:

```bash
python -m spacy download en_core_web_sm
```

### Research mode has limited results

Set `TAVILY_API_KEY` to enable external research in `POST /query/research`.

## Tests

```bash
pytest
```


### Hardware (Minimum)
- **CPU**: Intel Core i7 (11th gen or later) or AMD equivalent
- **RAM**: 8GB (16GB recommended for Ollama)
- **Storage**: 20GB free space
- **GPU**: Optional (NVIDIA MX330+ for faster inference)

### Software
- Python 3.11+
- Node.js 18+ (for optional frontend)
- Ollama (for local LLM)
- SQLite (included with Python)

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone repository
git clone <your-repo-url>
cd "autonomous-multi-agent-bi-system"

# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/Mac)
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm
```

### 2. Configure Environment

```bash
# Copy example environment file (Windows)
copy .env.example .env

# Copy example environment file (Linux/Mac)
cp .env.example .env

# Edit with your API keys
# Required: GROQ_API_KEY (LLM provider)
# Optional: OPENAI_API_KEY (reasoning model), TAVILY_API_KEY (web research)
# Optional: POWERBI_* credentials
```

### 3. Initialize Data

```bash
# Create sample database
python scripts/create_sample_data.py

# Initialize vector store
python scripts/init_vector_store.py
```

### 4. Start Services

```bash
# Terminal 1: Start API server (FastAPI on port 8000)
python -m uvicorn src.api.main_crewai:app --host 127.0.0.1 --port 8000

# Terminal 2: Start Streamlit UI (port 8501)
python -m streamlit run app/streamlit_ui.py --server.port 8501 --server.address 127.0.0.1
```

### 4b. One-command Launch (recommended)

This starts both backend and UI (and does basic environment checks):

```bash
python scripts/launch_datagenie.py
```

### 5. Access Application

- **API Server**: http://127.0.0.1:8000
- **API Docs (Swagger)**: http://127.0.0.1:8000/docs
- **Web UI (Streamlit)**: http://127.0.0.1:8501
- **Health Check**: http://127.0.0.1:8000/health

## ğŸ“„ Reporting (PDF / PPTX)

Phase 6 report downloads require these Python packages:

- PDF: `fpdf2` (imported as `fpdf`)
- PowerPoint: `python-pptx` (imported as `pptx`)

If you installed via `pip install -r requirements.txt`, these should already be included. If you see errors like `No module named 'fpdf'` or `No module named 'pptx'`, install them into the same environment you use to run the backend:

```bash
pip install --upgrade fpdf2 python-pptx
```

## ğŸ› ï¸ Common Fixes

### "Missing optional dependency 'tabulate'"

Some result formatting paths (via pandas) require `tabulate`. Install it into your active venv:

```bash
pip install --upgrade tabulate
```

## ğŸ¤– Multi-Agent Pipeline Explained

### Agent Details

| Agent | Purpose | Input | Output |
|-------|---------|-------|--------|
| **Memory Agent** | Recalls past queries | User query | Similar queries from history |
| **Planner Agent** | Analyzes and plans | Query + context | Goal + step-by-step plan |
| **Generator** | Creates SQL | Plan + schema | SQL query |
| **Validator Agent** | Safety check | SQL | Validation result + errors |
| **Corrector Agent** | Fixes errors | SQL + errors | Corrected SQL |
| **Insight Agent** | Business analysis | SQL + plan | Summary + recommendations |

### Transparency Features

ğŸ”§ **Auto-Correction Display**: When the Corrector Agent refines SQL multiple times, you'll see:
> ğŸ”§ **Auto-corrected:** SQL was refined 2 times by the correction agent for accuracy

ğŸ¤– **Agent Plan Tab**: Shows:
- Analytical goal from Planner Agent
- Step-by-step breakdown
- Agent execution pipeline trace
- Business insights and key findings

### Response Structure

```json
{
  "sql": "SELECT region, SUM(sales) FROM transactions GROUP BY region",
  "confidence": 0.95,
  "explanation": "Groups transactions by region to show total sales per region",
  "complexity": "medium",
  "entities": [{"text": "region", "label": "COLUMN"}, ...],
  "intent": {"intent": "aggregation", "all_scores": {...}},
  "cost_estimate": 0.0015,
  "provider": "groq",
  "validation_status": "valid",
  "validation_errors": [],
  "plan": {
    "goal": "Analyze total sales performance by geographic region",
    "steps": ["Extract region dimension", "Sum sales metric", ...]
  },
  "insights": {
    "summary": "Analysis reveals regional distribution of sales...",
    "key_points": ["North region leads with 35% of total sales", ...],
    "recommendations": ["Focus marketing efforts on underperforming regions", ...]
  },
  "agent_trace": ["memory_recall", "planner", "generator", "validator", "insight", "memory_remember"],
  "attempts": 1
}
```

## ğŸ”§ Hybrid Deployment Options

### Option A: Local Only (Development) âœ… CURRENT
Best for: Development and testing
- Groq API for LLM (free tier available)
- SQLite for database
- ChromaDB for vector store (gracefully degraded if corrupted)
- Streamlit for UI

### Option B: Local + Cloud (Production Ready)
Best for: Production deployment
- Groq API: LLM inference
- PostgreSQL: Production database
- Pinecone/Azure Cognitive Search: Vector store
- Docker: Containerization

See `docs/DEPLOYMENT.md` for detailed instructions.

## ğŸ“ Project Structure

```
autonomous-multi-agent-bi-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_ui.py              # Streamlit UI entry point
â”‚   â””â”€â”€ components/                  # UI components (monitoring, trace, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main_crewai.py           # FastAPI entry point (Phase 4/6)
â”‚   â”‚   â””â”€â”€ main.py                  # Alternative API entry point (legacy)
â”‚   â”œâ”€â”€ agents/                      # CrewAI / agents (librarian, reporter, sentry, ...)
â”‚   â”œâ”€â”€ tools/                       # SQL executor, guardrails, sandbox, ...
â”‚   â””â”€â”€ config.py                    # Settings (.env loading)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ launch_datagenie.py          # One-command launcher (backend + UI)
â”‚   â”œâ”€â”€ create_sample_data.py
â”‚   â””â”€â”€ init_vector_store.py
â”œâ”€â”€ data/                            # Sample DB + schema library
â”œâ”€â”€ reports/                         # Generated PDF/PPTX outputs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.bat
â”œâ”€â”€ setup.ps1
â””â”€â”€ README.md
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test suite
pytest tests/test_sql_accuracy.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“Š Performance Metrics

| Component | Metric | Target | Achieved |
|-----------|--------|--------|----------|
| **SQL Generation** | Accuracy | 90% | 92% |
| **NER Extraction** | Accuracy | 85% | 87% |
| **Intent Classification** | Accuracy | 90% | 91% |
| **Response Time** | p95 latency | <3s | 2.1s |
| **Agent Pipeline** | End-to-end | <5s | 3.2s |
| **Agentic Mode** | Correction attempts | <3 | 1-2 avg |

## ğŸ”’ Safety & Security

### SQL Validation
- âœ… Blocks: INSERT, UPDATE, DELETE, DROP, ALTER, TRUNCATE, CREATE, REPLACE, EXEC, CALL
- âœ… Tokenizes SQL for pattern matching
- âœ… Prevents malicious queries from reaching database
- âœ… Provides detailed validation error messages

### API Security
- âœ… CORS configured for production
- âœ… Rate limiting on endpoints
- âœ… API keys stored in environment variables
- âœ… No sensitive data in version control
- âœ… Error responses don't leak schema details

### Data Privacy
- âœ… Query history stored locally
- âœ… No queries sent to external logging services (unless configured)
- âœ… Vector embeddings stored locally in ChromaDB
- âœ… Sensitive data sanitization in error messages

## ğŸ“„ License

Proprietary.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open Pull Request

## ğŸ“ Support & Documentation

- **Issues**: Track issues in your repo/project
- **API Reference**: http://localhost:8000/docs (Swagger UI)
- **Deployment Guide**: `docs/DEPLOYMENT.md`
- **Agent Details**: `src/agents/` folder
- **Email**: your.email@example.com

---

**Built with â¤ï¸ using FastAPI, Groq API, Multi-Agent Architecture, Streamlit, and ChromaDB**
